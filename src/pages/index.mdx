---
# 页面元信息（按需修改）
title: Thinking with Sound — Demo
layout: "../layouts/BaseLayout.astro"
description: Thinking-with-Sound (TwS) equips LALMs with audio-domain Chain-of-Thought, enabling interleaved linguistic reasoning and on-the-fly acoustic analysis for robust multimodal understanding.
---

<section className="max-w-3xl mx-auto px-6 pt-16 pb-8">
  {/* 标题 */}
  <h1 className="text-4xl sm:text-5xl font-extrabold leading-tight tracking-tight text-gray-900 dark:text-white">
    <span className="mr-2">🎧</span>
    <span>Thinking with Sound: <span className="block">Audio Chain-of-Thought Enables Multimodal Reasoning in Large Audio‑Language Models</span></span>
  </h1>

  {/* 作者行（示例：可改为你的作者与链接）*/}
  <p className="mt-6 text-lg leading-7">
    <a href="#" className="text-blue-600 hover:underline">Zhen Xiong</a><sup>1</sup>,
    <span> </span><a href="#" className="text-blue-600 hover:underline">Yujun Cai</a><sup>2</sup>,
    <span> </span><a href="#" className="text-blue-600 hover:underline">Zhecheng Li</a><sup>3</sup>,
    <span> </span><a href="#" className="text-blue-600 hover:underline">Junsong Yuan</a><sup>4</sup>,
    <span> </span><a href="#" className="text-blue-600 hover:underline">Yiwei Wang</a><sup>5</sup>
  </p>
  <p className="mt-1 text-base text-gray-600 dark:text-gray-300">
    <sup>1</sup>University of Southern California, <sup>2</sup>University of Queensland,<br/>
    <sup>3</sup>University of California, San Diego, <sup>4</sup>University of Buffalo,<br/>
    <sup>5</sup>University of California, Merced
  </p>

  {/* 按钮区（Paper / arXiv / Code）*/}
  <div className="mt-6 flex flex-wrap gap-3">
    <a href="#" className="inline-flex items-center gap-2 rounded-md bg-gray-100 dark:bg-gray-800 px-3.5 py-2 text-sm font-medium text-gray-900 dark:text-gray-100 border border-gray-200 dark:border-gray-700">
      <span aria-hidden>📄</span>
      <span>Paper (coming soon)</span>
    </a>
    <a href="#" className="inline-flex items-center gap-2 rounded-md bg-gray-100 dark:bg-gray-800 px-3.5 py-2 text-sm font-medium text-gray-900 dark:text-gray-100 border border-gray-200 dark:border-gray-700">
      <span aria-hidden>✕</span>
      <span>arXiv (coming soon)</span>
    </a>
    <a href="#" className="inline-flex items-center gap-2 rounded-md bg-black text-white px-3.5 py-2 text-sm font-semibold hover:opacity-90">
      <span aria-hidden>💻</span>
      <span>Code</span>
    </a>
  </div>

  {/* 副标题/一句话简介 */}
  <p className="mt-6 text-lg text-gray-700 dark:text-gray-200">
    Thinking‑with‑Sound (TwS) equips LALMs with audio‑domain Chain‑of‑Thought, enabling interleaved linguistic reasoning and on‑the‑fly acoustic analysis for robust multimodal understanding.
  </p>
</section>

<hr className="max-w-3xl mx-auto border-gray-200 dark:border-gray-700"/>

<section className="max-w-3xl mx-auto px-6 pt-10 pb-24">
  <h2 className="text-2xl font-bold text-gray-900 dark:text-white">Abstract</h2>
  <p className="mt-4 leading-7 text-gray-800 dark:text-gray-200">
    Recent Large Audio‑Language Models (LALMs) have shown strong performance on various audio understanding tasks such as speech translation and Audio Q&amp;A. However, they exhibit significant limitations on challenging audio reasoning tasks in complex acoustic scenarios. These situations would greatly benefit from the use of acoustic tools like noise suppression, source separation, and precise temporal alignment, but current LALMs lack access to such tools. To address this limitation, we introduce <em>Thinking‑with‑Sound</em> (TwS), a framework that equips LALMs with Audio CoT by combining linguistic reasoning with on‑the‑fly audio‑domain analysis. Unlike existing approaches that treat audio as static input, TwS enables models to actively <strong>think</strong> with audio signals, performing numerical analysis and digital manipulation through multimodal reasoning. To evaluate this approach, we construct <strong>MELD‑Hard1k</strong>, a new robustness benchmark created by introducing various acoustic perturbations. Experiments reveal that state‑of‑the‑art LALMs suffer dramatic performance degradation on MELD‑Hard1k, with accuracy dropping by more than 50% compared to clean audio. TwS achieves substantial improvements in robustness, demonstrating both effectiveness and scalability: small models gain 24.73% absolute accuracy, with improvements scaling consistently up to 36.61% for larger models. Our findings demonstrate that Audio CoT can significantly enhance robustness without retraining, opening new directions for developing more robust audio understanding systems.
  </p>
</section>

{/* 轻量样式兜底（如果模板已接入 Tailwind 可忽略）*/}
<style>
  {`
  :root { --tw-shadow-color: 0 0 0; }
  section p a { text-decoration: underline; }
  `}
</style>
