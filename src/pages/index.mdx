---
# é¡µé¢å…ƒä¿¡æ¯ï¼ˆæŒ‰éœ€ä¿®æ”¹ï¼‰
title: Thinking with Sound â€” Demo
layout: "../layouts/BaseLayout.astro"
description: Thinking-with-Sound (TwS) equips LALMs with audio-domain Chain-of-Thought, enabling interleaved linguistic reasoning and on-the-fly acoustic analysis for robust multimodal understanding.
---

<section className="max-w-3xl mx-auto px-6 pt-16 pb-8">
  {/* æ ‡é¢˜ */}
  <h1 className="text-4xl sm:text-5xl font-extrabold leading-tight tracking-tight text-gray-900 dark:text-white">
    <span className="mr-2">ğŸ§</span>
    <span>Thinking with Sound: <span className="block">Audio Chain-of-Thought Enables Multimodal Reasoning in Large Audioâ€‘Language Models</span></span>
  </h1>

  {/* ä½œè€…è¡Œï¼ˆç¤ºä¾‹ï¼šå¯æ”¹ä¸ºä½ çš„ä½œè€…ä¸é“¾æ¥ï¼‰*/}
  <p className="mt-6 text-lg leading-7">
    <a href="#" className="text-blue-600 hover:underline">Zhen Xiong</a><sup>1</sup>,
    <span> </span><a href="#" className="text-blue-600 hover:underline">Yujun Cai</a><sup>2</sup>,
    <span> </span><a href="#" className="text-blue-600 hover:underline">Zhecheng Li</a><sup>3</sup>,
    <span> </span><a href="#" className="text-blue-600 hover:underline">Junsong Yuan</a><sup>4</sup>,
    <span> </span><a href="#" className="text-blue-600 hover:underline">Yiwei Wang</a><sup>5</sup>
  </p>
  <p className="mt-1 text-base text-gray-600 dark:text-gray-300">
    <sup>1</sup>University of Southern California, <sup>2</sup>University of Queensland,<br/>
    <sup>3</sup>University of California, San Diego, <sup>4</sup>University of Buffalo,<br/>
    <sup>5</sup>University of California, Merced
  </p>

  {/* æŒ‰é’®åŒºï¼ˆPaper / arXiv / Codeï¼‰*/}
  <div className="mt-6 flex flex-wrap gap-3">
    <a href="#" className="inline-flex items-center gap-2 rounded-md bg-gray-100 dark:bg-gray-800 px-3.5 py-2 text-sm font-medium text-gray-900 dark:text-gray-100 border border-gray-200 dark:border-gray-700">
      <span aria-hidden>ğŸ“„</span>
      <span>Paper (coming soon)</span>
    </a>
    <a href="#" className="inline-flex items-center gap-2 rounded-md bg-gray-100 dark:bg-gray-800 px-3.5 py-2 text-sm font-medium text-gray-900 dark:text-gray-100 border border-gray-200 dark:border-gray-700">
      <span aria-hidden>âœ•</span>
      <span>arXiv (coming soon)</span>
    </a>
    <a href="#" className="inline-flex items-center gap-2 rounded-md bg-black text-white px-3.5 py-2 text-sm font-semibold hover:opacity-90">
      <span aria-hidden>ğŸ’»</span>
      <span>Code</span>
    </a>
  </div>

  {/* å‰¯æ ‡é¢˜/ä¸€å¥è¯ç®€ä»‹ */}
  <p className="mt-6 text-lg text-gray-700 dark:text-gray-200">
    Thinkingâ€‘withâ€‘Sound (TwS) equips LALMs with audioâ€‘domain Chainâ€‘ofâ€‘Thought, enabling interleaved linguistic reasoning and onâ€‘theâ€‘fly acoustic analysis for robust multimodal understanding.
  </p>
</section>

<hr className="max-w-3xl mx-auto border-gray-200 dark:border-gray-700"/>

<section className="max-w-3xl mx-auto px-6 pt-10 pb-24">
  <h2 className="text-2xl font-bold text-gray-900 dark:text-white">Abstract</h2>
  <p className="mt-4 leading-7 text-gray-800 dark:text-gray-200">
    Recent Large Audioâ€‘Language Models (LALMs) have shown strong performance on various audio understanding tasks such as speech translation and Audio Q&amp;A. However, they exhibit significant limitations on challenging audio reasoning tasks in complex acoustic scenarios. These situations would greatly benefit from the use of acoustic tools like noise suppression, source separation, and precise temporal alignment, but current LALMs lack access to such tools. To address this limitation, we introduce <em>Thinkingâ€‘withâ€‘Sound</em> (TwS), a framework that equips LALMs with Audio CoT by combining linguistic reasoning with onâ€‘theâ€‘fly audioâ€‘domain analysis. Unlike existing approaches that treat audio as static input, TwS enables models to actively <strong>think</strong> with audio signals, performing numerical analysis and digital manipulation through multimodal reasoning. To evaluate this approach, we construct <strong>MELDâ€‘Hard1k</strong>, a new robustness benchmark created by introducing various acoustic perturbations. Experiments reveal that stateâ€‘ofâ€‘theâ€‘art LALMs suffer dramatic performance degradation on MELDâ€‘Hard1k, with accuracy dropping by more than 50% compared to clean audio. TwS achieves substantial improvements in robustness, demonstrating both effectiveness and scalability: small models gain 24.73% absolute accuracy, with improvements scaling consistently up to 36.61% for larger models. Our findings demonstrate that Audio CoT can significantly enhance robustness without retraining, opening new directions for developing more robust audio understanding systems.
  </p>
</section>

{/* è½»é‡æ ·å¼å…œåº•ï¼ˆå¦‚æœæ¨¡æ¿å·²æ¥å…¥ Tailwind å¯å¿½ç•¥ï¼‰*/}
<style>
  {`
  :root { --tw-shadow-color: 0 0 0; }
  section p a { text-decoration: underline; }
  `}
</style>
