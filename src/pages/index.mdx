---
# 页面元信息（按需修改）
title: Thinking with Sound — Demo
# 不使用自定义 layout，避免路径问题；如需可自行加：layout: "../layouts/BaseLayout.astro"
---

{/* 顶部 Hero，整体居中，与截图一致（无标题 icon） */}
<section className="max-w-3xl mx-auto px-6 pt-16 pb-8 text-center">
  <h1 className="text-4xl sm:text-5xl font-extrabold leading-tight tracking-tight text-gray-900 dark:text-white">
    Thinking with Sound: <span className="block">Audio Chain-of-Thought Enables Multimodal Reasoning in Large Audio-Language Models</span>
  </h1>

  {/* 作者（带链接） */}
  <p className="mt-6 text-lg leading-7">
    <a href="#" className="text-blue-600 hover:underline">Zhen Xiong</a><sup>1</sup>,
    <span> </span><a href="#" className="text-blue-600 hover:underline">Yujun Cai</a><sup>2</sup>,
    <span> </span><a href="#" className="text-blue-600 hover:underline">Zhecheng Li</a><sup>3</sup>,
    <span> </span><a href="#" className="text-blue-600 hover:underline">Junsong Yuan</a><sup>4</sup>,
    <span> </span><a href="#" className="text-blue-600 hover:underline">Yiwei Wang</a><sup>5</sup>
  </p>

  {/* 单位列表 */}
  <p className="mt-1 text-base text-gray-600 dark:text-gray-300">
    <sup>1</sup>University of Southern California, <sup>2</sup>University of Queensland,<br/>
    <sup>3</sup>University of California, San Diego, <sup>4</sup>University of Buffalo,<br/>
    <sup>5</sup>University of California, Merced
  </p>

  {/* 三个按钮：Paper / arXiv / Code（与截图一致的顺序与风格） */}
  <div className="mt-6 flex flex-wrap items-center justify-center gap-3">
    <a href="#" className="inline-flex items-center gap-2 rounded-md bg-gray-100 dark:bg-gray-800 px-3.5 py-2 text-sm font-medium text-gray-900 dark:text-gray-100 border border-gray-200 dark:border-gray-700">
      <span>Paper (coming soon)</span>
    </a>
    <a href="#" className="inline-flex items-center gap-2 rounded-md bg-gray-100 dark:bg-gray-800 px-3.5 py-2 text-sm font-medium text-gray-900 dark:text-gray-100 border border-gray-200 dark:border-gray-700">
      <span>arXiv (coming soon)</span>
    </a>
    <a href="#" className="inline-flex items-center gap-2 rounded-md bg-black text-white px-3.5 py-2 text-sm font-semibold hover:opacity-90">
      <span>Code</span>
    </a>
  </div>

  {/* 一句话简介 */}
  <p className="mt-6 text-lg text-gray-700 dark:text-gray-200">
    Thinking-with-Sound (TwS) equips LALMs with audio-domain Chain-of-Thought, enabling interleaved linguistic reasoning and on-the-fly acoustic analysis for robust multimodal understanding.
  </p>
</section>

<hr className="max-w-3xl mx-auto border-gray-200 dark:border-gray-700"/>

{/* Abstract 区域：标题居中、正文常规对齐 */}
<section className="max-w-3xl mx-auto px-6 pt-10 pb-24">
  <h2 className="text-3xl font-bold text-center text-gray-900 dark:text-white">Abstract</h2>
  <p className="mt-6 leading-7 text-gray-800 dark:text-gray-200">
    Recent Large Audio-Language Models (LALMs) have shown strong performance on various audio understanding tasks such as speech translation and Audio Q&amp;A. However, they exhibit significant limitations on challenging audio reasoning tasks in complex acoustic scenarios. These situations would greatly benefit from the use of acoustic tools like noise suppression, source separation, and precise temporal alignment, but current LALMs lack access to such tools. To address this limitation, we introduce <em>Thinking-with-Sound</em> (TwS), a framework that equips LALMs with Audio CoT by combining linguistic reasoning with on-the-fly audio-domain analysis. Unlike existing approaches that treat audio as static input, TwS enables models to actively <strong>think</strong> with audio signals, performing numerical analysis and digital manipulation through multimodal reasoning. To evaluate this approach, we construct <strong>MELD-Hard1k</strong>, a new robustness benchmark created by introducing various acoustic perturbations. Experiments reveal that state-of-the-art LALMs suffer dramatic performance degradation on MELD-Hard1k, with accuracy dropping by more than 50% compared to clean audio. TwS achieves substantial improvements in robustness, demonstrating both effectiveness and scalability: small models gain 24.73% absolute accuracy, with improvements scaling consistently up to 36.61% for larger models. Our findings demonstrate that Audio CoT can significantly enhance robustness without retraining, opening new directions for developing more robust audio understanding systems.
  </p>
</section>
