---
layout: ../layouts/Layout.astro
title: "Thinking with Sound: Audio Chain-of-Thought Enables Multimodal Reasoning in Large Audio-Language Models"
description: "Thinking-with-Sound (TwS) equips LALMs with audio-domain Chain-of-Thought, enabling interleaved linguistic reasoning and on-the-fly acoustic analysis for robust multimodal understanding."
favicon: favicon.svg
thumbnail: screenshot-light.png
---

import Header from "../components/Header.astro";
import Video from "../components/Video.astro";
import HighlightedSection from "../components/HighlightedSection.astro";
import SmallCaps from "../components/SmallCaps.astro";
import Figure from "../components/Figure.astro";
import Image from "../components/Image.astro";
import TwoColumns from "../components/TwoColumns.astro";
import YouTubeVideo from "../components/YouTubeVideo.astro";
import LaTeX from "../components/LaTeX.astro";

import { ImageComparison } from "../components/ImageComparison.tsx";

import outside from "../assets/outside.mp4";
import transformer from "../assets/transformer.webp";
import Splat from "../components/Splat.tsx"
import dogsDiffc from "../assets/dogs-diffc.png"
import dogsTrue from "../assets/dogs-true.png"


import CodeBlock from "../components/CodeBlock.astro";
import Table from "../components/Table.astro";
export const components = {pre: CodeBlock, table: Table}

<Header
  title={frontmatter.title}
  authors={[
    {
      name: "Zhen Xiong",
      institution: "University of Southern California",
      notes: ["¹"],
    },
    {
      name: "Yujun Cai",
      institution: "University of Queensland",
      notes: ["²"],
    },
    {
      name: "Zhecheng Li",
      institution: "University of California, San Diego",
      notes: ["³"],
    },
    {
      name: "Junsong Yuan",
      institution: "University of Buffalo",
      notes: ["⁴"],
    },
    {
      name: "Yiwei Wang",
      institution: "University of California, Merced",
      notes: ["⁵"],
    },
  ]}
  links={[
    {
      name: "Paper (coming soon)",
      url: "#",
      icon: "ri:file-pdf-2-line",
    },
    {
      name: "arXiv (coming soon)",
      url: "#",
      icon: "academicons:arxiv",
    },
    {
      name: "Code",
      url: "#",
      icon: "ri:github-line",
    },
  ]}
  />

<div class="text-center text-lg mb-8">
  <strong>Thinking-with-Sound (TwS)</strong> equips LALMs with audio-domain Chain-of-Thought, enabling interleaved linguistic reasoning and on-the-fly acoustic analysis for robust multimodal understanding.
</div>

<Video source={outside} />

<HighlightedSection>

## Abstract

Large Audio-Language Models (LALMs) have demonstrated remarkable capabilities in audio understanding and generation. However, their reasoning processes remain opaque, and they often struggle with complex audio reasoning tasks that require multi-step analysis. To address these limitations, we introduce **Thinking-with-Sound (TwS)**, a novel framework that equips LALMs with audio-domain Chain-of-Thought (CoT) reasoning capabilities. TwS enables models to perform interleaved linguistic reasoning and on-the-fly acoustic analysis, leading to more robust and interpretable multimodal understanding. Our approach leverages a dual-stream architecture that processes both textual and acoustic features simultaneously, allowing the model to reason about audio content through natural language while maintaining access to low-level acoustic details. We evaluate TwS on a comprehensive suite of audio reasoning tasks, including audio question answering, audio summarization, and audio-based decision making. Experimental results demonstrate that TwS significantly improves performance on complex audio reasoning tasks, with particular gains on the challenging **MELD-Hard1k** benchmark. Furthermore, our **Audio CoT** approach provides interpretable reasoning traces that reveal how the model processes and understands audio content. The framework is generalizable across different LALM architectures and can be easily integrated into existing systems. Our work opens new directions for developing more capable and transparent audio-language models that can reason about sound in a human-like manner.

</HighlightedSection>

## Figures

Use the figure component to display images, videos, equations, or any other element, with an optional caption.

<Figure>
  <Image slot="figure" source={transformer} altText="Diagram of the transformer deep learning architecture." />
  <span slot="caption">Diagram of the transformer deep learning architecture.</span>
</Figure>

## Image comparison slider

An interactive, accessible slider component with keyboard navigation.
<Figure>
  <ImageComparison slot="figure" client:load imageUrlOne={dogsDiffc.src} imageUrlTwo={dogsTrue.src} altTextOne="Photo of two dogs running side-by-side in shallow water, lossily compressed using the DiffC algorithm" altTextTwo="Original photo of two dogs running side-by-side in shallow water" />
  <span slot="caption">A photo of two dogs running side-by-side in shallow water, lossily compressed using the <a href="https://jeremyiv.github.io/diffc-project-page/">DiffC algorithm</a>.</span>
</Figure>

## Two columns

Use the two columns component to display two columns of content. In this example, the first column contains a figure with a YouTube video and the second column contains a figure with a custom [React](https://react.dev/) component. By default, they display side by side, but if the screen is narrow enough (for example, on mobile), they're arranged vertically.

<TwoColumns>
  <Figure slot="left">
    <YouTubeVideo slot="figure" videoId="wjZofJX0v4M" />
    <span slot="caption">Take a look at this YouTube video.</span>
  </Figure>
  <Figure slot="right">
    <Splat slot="figure" client:idle />
    <span slot="caption">Now look at this <a href="https://en.wikipedia.org/wiki/Gaussian_splatting">Gaussian splat</a>, rendered with a React component.</span>
  </Figure>
</TwoColumns>

## Heading levels

Use headings to divide your content into sections.

### Heading 3

Go down a level to heading 3...

#### Heading 4

...and down again to heading 4.

## LaTeX

You can also add LaTeX formulas, rendered during the build process using [KaTeX](https://katex.org/) so they're quick to load for visitors of your project page. You can write them inline, like this: <LaTeX inline formula="a^2 + b^2 = c^2" />. Or, you can write them as a block:

<LaTeX formula="\int_a^b f(x) dx" />

## Tables

You can add simple tables using [GitHub Flavored Markdown syntax](https://docs.github.com/en/get-started/writing-on-github/working-with-advanced-formatting/organizing-information-with-tables):

| Model | Accuracy | F1 score | Training time (hours) |
| :--- | :---: | :---: | :---: |
| BERT-base | 0.89 | 0.87 | 4.5 |
| RoBERTa-large | 0.92 | 0.91 | 7.2 |
| DistilBERT | 0.86 | 0.84 | 2.1 |
| XLNet | 0.90 | 0.89 | 6.8 |

## BibTeX citation

```bibtex
@misc{roman2024academic,
  author = "{Roman Hauksson}",
  title = "Academic Project Page Template",
  year = "2024",
  howpublished = "\url{https://research-template.roman.technology}",
}
```
