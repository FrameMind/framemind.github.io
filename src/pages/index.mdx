---
layout: ../layouts/Layout.astro
title: "FrameMind: Frame-Interleaved Video Reasoning via Reinforcement Learning"
description: "We propose FrameMind, an RL-trained framework that learns to actively sample video frames while it reasons. This dynamic perception overcomes the limits of static models and achieves state-of-the-art results."
favicon: favicon.svg
thumbnail: screenshot-light.png
---

import Header from "../components/Header.astro";
import Video from "../components/Video.astro";
import HighlightedSection from "../components/HighlightedSection.astro";
import SmallCaps from "../components/SmallCaps.astro";
import Figure from "../components/Figure.astro";
import Image from "../components/Image.astro";
import TwoColumns from "../components/TwoColumns.astro";
import YouTubeVideo from "../components/YouTubeVideo.astro";
import LaTeX from "../components/LaTeX.astro";

import { ImageComparison } from "../components/ImageComparison.tsx";

import outside from "../assets/outside.mp4";
import transformer from "../assets/transformer.webp";
import Splat from "../components/Splat.tsx"
import dogsDiffc from "../assets/dogs-diffc.png"
import dogsTrue from "../assets/dogs-true.png"
import overview from "../assets/overview9.png"
import CodeBlock from "../components/CodeBlock.astro";
import Table from "../components/Table.astro";
export const components = {pre: CodeBlock, table: Table}

<Header
  title={frontmatter.title}
  authors={[
    {
      name: "Haonan Ge",
      url: "https://johnny040216.github.io",
      institution: "University of California, Merced",
      notes: ["¹"],
    },
    {
      name: "Yiwei Wang",
      url: "https://wangywust.github.io/",
      institution: "University of California, Merced",
      notes: ["¹"],
    },
    {
      name: "Kai-Wei Chang",
      url: "https://web.cs.ucla.edu/~kwchang/",
      institution: "University of California, Los Angeles",
      notes: ["²"],
    },
    {
      name: "Hang Wu",
      url: "https://wuhang03.github.io/",
      institution: "University of California, Merced",
      notes: ["¹"],
    },
    {
      name: "Yujun Cai",
      url: "https://vanoracai.github.io/",
      institution: "The University of Queensland",
      notes: ["³"],
    },
  ]}
  links={[
    {
      name: "Paper",
      url: "https://www.arxiv.org/pdf/2509.24008",
      icon: "ri:file-pdf-2-line",
    },
    {
      name: "arXiv (coming soon)",
      url: "https://www.arxiv.org/abs/2509.24008",
      icon: "academicons:arxiv",
    },
    {
      name: "Code (coming soon)",
      url: "#",
      icon: "ri:github-line",
    },
  ]}
  />

<div class="text-center text-lg mb-8">
 We propose <strong>FrameMind</strong>, an RL-trained framework that learns to actively sample video frames while it reasons. This dynamic perception overcomes the limits of static models and achieves state-of-the-art results.
</div>


<HighlightedSection>

## Abstract

Current video understanding models rely on fixed frame sampling strategies, processing predetermined visual inputs regardless of the specific reasoning requirements of each question. This static approach limits their ability to adaptively gather visual evidence, leading to suboptimal performance on tasks requiring either broad temporal coverage or fine-grained spatial detail. In this paper, we introduce FrameMind, a novel end-to-end framework trained with reinforcement learning that enables models to dynamically request visual information during reasoning through Frame-Interleaved Chain-of-Thought (FiCOT). Unlike traditional approaches, FrameMind operates in multiple turns where the model alternates between textual reasoning and active visual perception, using tools to extract targeted frames or video clips based on identified knowledge gaps. To train effective dynamic sampling policies, we propose Dynamic Resolution Frame Sampling (DRFS), which exposes models to diverse temporal–spatial trade-offs during learning, and DRFS-GRPO, a group-relative policy optimization algorithm that learns from outcome-based rewards without requiring frame-level annotations. Extensive experiments on challenging benchmarks like MLVU and VideoMME demonstrate that our method significantly outperforms existing models, advancing the state of the art in flexible and efficient video understanding.

</HighlightedSection>

## Overview

<Figure>
  <Image slot="figure" source={overview} altText="Overall framework of FrameMind, illustrating the iterative perception-reasoning loop. The agent first thinks, then acts (calls tools) to gather visual evidence, and updates its understanding to inform the next cycle." />
  <span slot="caption">Overall framework of FrameMind, illustrating the iterative perception-reasoning loop. The agent first thinks, then acts (calls tools) to gather visual evidence, and updates its understanding to inform the next cycle.</span>
</Figure>


## BibTeX citation

```bibtex
@misc{ge2025famemindframeinterleavedvideoreasoning,
      title={FameMind: Frame-Interleaved Video Reasoning via Reinforcement Learning}, 
      author={Haonan Ge and Yiwei Wang and Kai-Wei Chang and Hang Wu and Yujun Cai},
      year={2025},
      eprint={2509.24008},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2509.24008}, 
}}
```
